{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "typical-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from functools import partial\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-equilibrium",
   "metadata": {},
   "source": [
    "### Glorot and He Initialization\n",
    "\n",
    "$$ \\mathrm{Normal\\ distribution\\ with\\ mean\\ 0\\ and\\ variance\\ } \\sigma^{2} = \\frac{1}{fan_{\\mathrm{avg}}} $$\n",
    "\n",
    "$$ \\mathrm{Or\\ a\\ uniform\\ distribution\\ between\\ } -r\\ \\mathrm{and\\ } +r\\ \\mathrm{,\\ with\\ } r = \\sqrt{\\frac{3}{fan_{\\mathrm{avg}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "invalid-helen",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x1475e7760>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(10, activation='relu', kernel_initializer='he_normal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "applicable-victor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x147e01a90>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "he_avg_init = keras.initializers.VarianceScaling(scale=2, mode='fan_avg', distribution='uniform')\n",
    "keras.layers.Dense(10, activation='sigmoid', kernel_initializer=he_avg_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-filename",
   "metadata": {},
   "source": [
    "### Nonsaturating Activation Functions\n",
    "\n",
    "$$ \\mathrm{ELU}_{\\alpha}(z) = $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "discrete-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[8]),\n",
    "    keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.LeakyReLU(alpha=0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "generous-reality",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[8]),\n",
    "    keras.layers.Dense(10, kernel_initializer='he_normal'),\n",
    "    keras.layers.PReLU()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "committed-atmosphere",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.InputLayer(input_shape=[8]),\n",
    "    keras.layers.Dense(10, kernel_initializer='lecun_normal', activation='selu')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "large-massachusetts",
   "metadata": {},
   "source": [
    "### Batch Normalization\n",
    "\n",
    "$$ 1.\\ \\mathbf{\\mu}_{B} = \\frac{1}{m_{B}}\\sum_{i = 1}^{m_{B}} \\mathbf{x}^{(i)}$$\n",
    "\n",
    "$$ 2.\\ \\mathbf{\\sigma}_{B}^{2} = \\frac{1}{m_{B}}\\sum_{i = 1}^{m_{B}} (\\mathbf{x}^{(i)} - \\mathbf{\\mu}_{B})^{2}$$\n",
    "\n",
    "$$ 3.\\ \\widehat{\\mathbf{x}}^{(i)} = \\frac{\\mathbf{x}^{(i)} - \\mathbf{\\mu}_{B}}{\\sqrt{\\mathbf{\\sigma}_{B}^{2} + \\epsilon}}$$\n",
    "\n",
    "$$ 4.\\ \\mathbf{z}^{(i)} = \\mathbf{\\gamma} \\otimes \\widehat{\\mathbf{x}}^{(i)} + \\mathbf{\\beta}$$\n",
    "\n",
    "### Implementing Batch Normalization with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "unavailable-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal'),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "subsequent-darwin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 784)               3136      \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 300)               235500    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 300)               1200      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 100)               400       \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 271,346\n",
      "Trainable params: 268,978\n",
      "Non-trainable params: 2,368\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "meaningful-necklace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batch_normalization/gamma:0', True),\n",
       " ('batch_normalization/beta:0', True),\n",
       " ('batch_normalization/moving_mean:0', False),\n",
       " ('batch_normalization/moving_variance:0', False)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(var.name, var.trainable) for var in model.layers[1].variables]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "current-strengthening",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py:1402: UserWarning: `layer.updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  warnings.warn('`layer.updates` will be removed in a future version. '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[1].updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "solved-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', use_bias=False),\n",
    "    keras.layers.BatchNormalization(),\n",
    "    keras.layers.Activation('elu'),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-moderator",
   "metadata": {},
   "source": [
    "$$ \\widehat{\\mathbf{v}} \\leftarrow  \\widehat{\\mathbf{v}}  \\times \\mathrm{momentum} + \\mathbf{v} \\times (1 - \\mathrm{momentum}) $$\n",
    "\n",
    "### Gradient Clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "approved-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(clipvalue=1.0)\n",
    "model.compile(loss='mse', optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "serial-paintball",
   "metadata": {},
   "source": [
    "### Transfer Learning with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "rough-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A = keras.models.load_model('my_mnist_model.h5')\n",
    "model_B_on_A = keras.models.Sequential(model_A.layers[:-1])\n",
    "model_B_on_A.add(keras.layers.Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "nuclear-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_A_clone = keras.models.clone_model(model_A)\n",
    "model_A_clone.set_weights(model_A.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "copyrighted-pipeline",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_B_on_A.layers[:-1]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "minus-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_B_on_A.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "random-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "weird-diamond",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_full_B = (y_train_full == 1).astype(int)\n",
    "y_test_B = (y_test == 1).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "vietnamese-springer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 8.4818 - accuracy: 0.9921 - val_loss: 3.1329 - val_accuracy: 0.9975\n",
      "Epoch 2/16\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 1.8224 - accuracy: 0.9981 - val_loss: 3.2045 - val_accuracy: 0.9974\n",
      "Epoch 3/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5032 - accuracy: 0.9978 - val_loss: 2.8456 - val_accuracy: 0.9970\n",
      "Epoch 4/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5206 - accuracy: 0.9975 - val_loss: 2.7176 - val_accuracy: 0.9976\n",
      "Epoch 5/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.4392 - accuracy: 0.9979 - val_loss: 3.6010 - val_accuracy: 0.9965\n",
      "Epoch 6/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.7307 - accuracy: 0.9975 - val_loss: 3.3078 - val_accuracy: 0.9977\n",
      "Epoch 7/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.2588 - accuracy: 0.9983 - val_loss: 3.4449 - val_accuracy: 0.9971\n",
      "Epoch 8/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.1769 - accuracy: 0.9984 - val_loss: 2.9763 - val_accuracy: 0.9969\n",
      "Epoch 9/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.2236 - accuracy: 0.9983 - val_loss: 2.7560 - val_accuracy: 0.9978\n",
      "Epoch 10/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.8422 - accuracy: 0.9978 - val_loss: 2.6236 - val_accuracy: 0.9978\n",
      "Epoch 11/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.2067 - accuracy: 0.9983 - val_loss: 2.4697 - val_accuracy: 0.9976\n",
      "Epoch 12/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.9066 - accuracy: 0.9985 - val_loss: 4.4416 - val_accuracy: 0.9956\n",
      "Epoch 13/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.0761 - accuracy: 0.9982 - val_loss: 2.9902 - val_accuracy: 0.9973\n",
      "Epoch 14/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.2031 - accuracy: 0.9985 - val_loss: 3.9879 - val_accuracy: 0.9967\n",
      "Epoch 15/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.5463 - accuracy: 0.9982 - val_loss: 2.9061 - val_accuracy: 0.9973\n",
      "Epoch 16/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 1.0862 - accuracy: 0.9983 - val_loss: 2.8295 - val_accuracy: 0.9976\n"
     ]
    }
   ],
   "source": [
    "history = model_B_on_A.fit(X_train_full, y_train_full_B, epochs=16, validation_data=(X_test, y_test_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "interesting-poverty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 0s 1ms/step - loss: 2.8295 - accuracy: 0.9976\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.8294708728790283, 0.9976000189781189]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_B_on_A.evaluate(X_test, y_test_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "delayed-charter",
   "metadata": {},
   "source": [
    "### Momentum Optimisation\n",
    "\n",
    "$$ \\mathbf{m} \\leftarrow \\beta\\ \\mathbf{m} - \\eta \\nabla_{\\theta} J({\\mathbf{\\theta}}) $$\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta + \\mathbf{m} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "terminal-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-warner",
   "metadata": {},
   "source": [
    "### Nesterov Accelerated Gradient\n",
    "\n",
    "$$ \\mathbf{m} \\leftarrow \\beta\\ \\mathbf{m} - \\eta \\nabla_{\\theta} J({\\mathbf{\\theta}} + \\beta\\ \\mathbf{m}) $$\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta + \\mathbf{m} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "incorporate-nicaragua",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.001, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-internship",
   "metadata": {},
   "source": [
    "### AdaGrad\n",
    "\n",
    "$$ \\mathbf{s} \\leftarrow \\mathbf{s} + \\nabla_{\\theta} J({\\mathbf{\\theta}}) \\otimes \\nabla_{\\theta} J({\\mathbf{\\theta}} )  $$\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J({\\mathbf{\\theta}}) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$\n",
    "\n",
    "### RMSProp\n",
    "\n",
    "$$ \\mathbf{s} \\leftarrow \\beta\\mathbf{s} + (1 - \\beta)\\nabla_{\\theta} J({\\mathbf{\\theta}}) \\otimes \\nabla_{\\theta} J({\\mathbf{\\theta}} )  $$\n",
    "\n",
    "$$ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} J({\\mathbf{\\theta}}) \\oslash \\sqrt{\\mathbf{s} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "genetic-evidence",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-worcester",
   "metadata": {},
   "source": [
    "### Adam and Nadam Optimisation\n",
    "\n",
    "\n",
    "$$ 1.\\ \\mathbf{m} \\leftarrow \\beta_{1}\\ \\mathbf{m} - (1 - \\beta_{1})\\eta \\nabla_{\\theta} J({\\mathbf{\\theta}}) $$\n",
    "\n",
    "\n",
    "$$ 2.\\ \\mathbf{s} \\leftarrow \\beta_{2}\\mathbf{s} + (1 - \\beta_{2})\\nabla_{\\theta} J({\\mathbf{\\theta}}) \\otimes \\nabla_{\\theta} J({\\mathbf{\\theta}} )  $$\n",
    "\n",
    "$$ 3.\\ \\widehat{\\mathbf{m}} \\leftarrow \\frac{\\mathbf{m}}{1 - \\beta_{1}^{\\top}} $$\n",
    "\n",
    "$$ 4.\\ \\widehat{\\mathbf{s}} \\leftarrow \\frac{\\mathbf{s}}{1 - \\beta_{2}^{\\top}} $$\n",
    "\n",
    "$$ 5.\\ \\theta \\leftarrow \\theta + \\eta \\widehat{\\mathbf{m}} \\oslash \\sqrt{\\widehat{\\mathbf{s}} + \\epsilon} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "needed-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-george",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "determined-rocket",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, decay=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "attached-advertising",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch):\n",
    "    return 0.01 * 0.1 ** (epoch / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "better-framing",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay(lr0, s):\n",
    "    def exponential_decay_fn(epoch):\n",
    "        return lr0 * 0.1 ** (epoch / s)\n",
    "    return exponential_decay_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "active-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "exponential_decay_fn = exponential_decay(lr0 = 0.01, s = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "thirty-creator",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 40547578280678329942016.0000 - accuracy: 0.0989 - val_loss: 2.3021 - val_accuracy: 0.0974\n",
      "Epoch 2/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3016 - accuracy: 0.1103 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 3/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 13/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 14/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 15/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 16/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model_A.fit(X_train_full, y_train_full, epochs=16, validation_data=(X_test, y_test), callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "spread-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_decay_fn(epoch, lr):\n",
    "    return lr * 0.1 ** (1 / 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "taken-smart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/16\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 13/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 14/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 15/16\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 16/16\n",
      "1875/1875 [==============================] - 5s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = keras.callbacks.LearningRateScheduler(exponential_decay_fn)\n",
    "history = model_A.fit(X_train_full, y_train_full, epochs=16, validation_data=(X_test, y_test), callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "received-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "def piecewise_constant_fn(epoch):\n",
    "    if epoch < 5:\n",
    "        return 0.01\n",
    "    elif epoch < 15:\n",
    "        return 0.005\n",
    "    else:\n",
    "        return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "phantom-closing",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "seventh-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 20 * len(X_train_full) // 32 # number of steps in 20 epochs, batch size 32\n",
    "learning_rate = keras.optimizers.schedules.ExponentialDecay(0.01, s, 0.1)\n",
    "optimizer = keras.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "governmental-oklahoma",
   "metadata": {},
   "source": [
    "### l<sub>1</sub> and l<sub>2</sub> Regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "generous-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', \n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "muslim-deputy",
   "metadata": {},
   "outputs": [],
   "source": [
    "RegularizedDense = partial(keras.layers.Dense,\n",
    "                           activation='elu',\n",
    "                           kernel_initializer='he_normal',\n",
    "                           kernel_regularizer=keras.regularizers.l2(0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "heated-tonight",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    RegularizedDense(300),\n",
    "    RegularizedDense(100),\n",
    "    RegularizedDense(10, activation='softmax', kernel_initializer='glorot_uniform')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "passive-ground",
   "metadata": {},
   "source": [
    "### Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "accepting-liberty",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(300, kernel_initializer='he_normal', activation='elu'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(100, kernel_initializer='he_normal', activation='elu'),\n",
    "    keras.layers.Dropout(rate=0.2),\n",
    "    keras.layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "injured-impression",
   "metadata": {},
   "source": [
    "### Monte Carlo (MC) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "governing-sitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probas = np.stack([model(X_test, training=True) for sample in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "altered-integrity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.99999996e-02, 5.39733889e-03, 6.10609436e-13, ...,\n",
       "        0.00000000e+00, 1.43050998e-01, 7.33125627e-01],\n",
       "       [0.00000000e+00, 1.08597618e-36, 0.00000000e+00, ...,\n",
       "        0.00000000e+00, 3.11710924e-01, 4.59130138e-01],\n",
       "       [3.55079334e-11, 2.98578236e-02, 2.55860943e-18, ...,\n",
       "        0.00000000e+00, 1.33081317e-01, 4.02926832e-01],\n",
       "       ...,\n",
       "       [0.00000000e+00, 3.99184339e-02, 1.69609950e-06, ...,\n",
       "        0.00000000e+00, 2.49999925e-01, 5.29253960e-01],\n",
       "       [8.01325381e-24, 9.27661452e-03, 9.99999978e-03, ...,\n",
       "        0.00000000e+00, 5.02125062e-02, 5.10198832e-01],\n",
       "       [0.00000000e+00, 3.38901231e-40, 1.72581105e-03, ...,\n",
       "        0.00000000e+00, 1.23382494e-01, 3.76955211e-01]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = y_probas.mean(axis=0)\n",
    "y_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "typical-forth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(model.predict(X_test[:1]), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "unexpected-startup",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[1.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.26, 0.74]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.54, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.46]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.84, 0.  , 0.  , 0.  , 0.  , 0.  , 0.16]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.04, 0.96]],\n",
       "\n",
       "       [[0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_probas[:, :1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bulgarian-transmission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.02, 0.01, 0.  , 0.01, 0.01, 0.07, 0.01, 0.  , 0.14, 0.73]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(y_proba[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "greatest-worry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14, 0.05, 0.  , 0.08, 0.1 , 0.26, 0.1 , 0.  , 0.35, 0.44]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_std = y_probas.std(axis=0)\n",
    "np.round(y_std[:1], 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "convertible-general",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCDropout(keras.layers.Dropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breathing-closure",
   "metadata": {},
   "source": [
    "### Max-Norm Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "precise-modern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.layers.core.Dense at 0x14e655910>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.layers.Dense(100, activation='elu', kernel_initializer='he_normal', \n",
    "                   kernel_constraint=keras.constraints.max_norm(1.))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-drove",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "#### 8.\n",
    "##### a."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "dangerous-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 activation=\"elu\",\n",
    "                                 kernel_initializer=\"he_normal\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-sunday",
   "metadata": {},
   "source": [
    "##### b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "straight-material",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "young-angel",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(lr=5e-5)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "peripheral-framework",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "170500096/170498071 [==============================] - 742s 4us/step\n"
     ]
    }
   ],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "X_train = X_train_full[5000:]\n",
    "y_train = y_train_full[5000:]\n",
    "X_valid = X_train_full[:5000]\n",
    "y_valid = y_train_full[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "greater-bahamas",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "refined-ethnic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 26s 15ms/step - loss: 9.2694 - accuracy: 0.1383 - val_loss: 2.1487 - val_accuracy: 0.2328\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 22s 15ms/step - loss: 2.0830 - accuracy: 0.2428 - val_loss: 2.0474 - val_accuracy: 0.2336\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.9477 - accuracy: 0.2926 - val_loss: 2.0591 - val_accuracy: 0.2596\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.8598 - accuracy: 0.3228 - val_loss: 1.8694 - val_accuracy: 0.3296\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.7980 - accuracy: 0.3458 - val_loss: 1.7851 - val_accuracy: 0.3404\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7453 - accuracy: 0.3670 - val_loss: 1.7687 - val_accuracy: 0.3514\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.7072 - accuracy: 0.3775 - val_loss: 1.7417 - val_accuracy: 0.3666\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.6706 - accuracy: 0.3971 - val_loss: 1.6819 - val_accuracy: 0.3940\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.6464 - accuracy: 0.4081 - val_loss: 1.6519 - val_accuracy: 0.3978\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6123 - accuracy: 0.4177 - val_loss: 1.6771 - val_accuracy: 0.3934\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5908 - accuracy: 0.4244 - val_loss: 1.6368 - val_accuracy: 0.4062\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5732 - accuracy: 0.4336 - val_loss: 1.6254 - val_accuracy: 0.4150\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.5512 - accuracy: 0.4401 - val_loss: 1.6242 - val_accuracy: 0.4088\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.5247 - accuracy: 0.4483 - val_loss: 1.6088 - val_accuracy: 0.4218\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5227 - accuracy: 0.4520 - val_loss: 1.5685 - val_accuracy: 0.4364\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.5027 - accuracy: 0.4599 - val_loss: 1.5915 - val_accuracy: 0.4292\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4842 - accuracy: 0.4640 - val_loss: 1.5929 - val_accuracy: 0.4316\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4753 - accuracy: 0.4715 - val_loss: 1.5775 - val_accuracy: 0.4342\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.4525 - accuracy: 0.4792 - val_loss: 1.5716 - val_accuracy: 0.4340\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.4454 - accuracy: 0.4770 - val_loss: 1.5746 - val_accuracy: 0.4422\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4402 - accuracy: 0.4821 - val_loss: 1.5853 - val_accuracy: 0.4378\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.4282 - accuracy: 0.4878 - val_loss: 1.5413 - val_accuracy: 0.4470\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4051 - accuracy: 0.4947 - val_loss: 1.5502 - val_accuracy: 0.4442\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4059 - accuracy: 0.4948 - val_loss: 1.5750 - val_accuracy: 0.4394\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3922 - accuracy: 0.4994 - val_loss: 1.5383 - val_accuracy: 0.4524\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3674 - accuracy: 0.5097 - val_loss: 1.6078 - val_accuracy: 0.4280\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3679 - accuracy: 0.5081 - val_loss: 1.5334 - val_accuracy: 0.4570\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3561 - accuracy: 0.5128 - val_loss: 1.5736 - val_accuracy: 0.4482\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3481 - accuracy: 0.5162 - val_loss: 1.5368 - val_accuracy: 0.4560\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3495 - accuracy: 0.5167 - val_loss: 1.5557 - val_accuracy: 0.4594\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3502 - accuracy: 0.5154 - val_loss: 1.5754 - val_accuracy: 0.4494\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3162 - accuracy: 0.5312 - val_loss: 1.5384 - val_accuracy: 0.4582\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3225 - accuracy: 0.5260 - val_loss: 1.5438 - val_accuracy: 0.4658\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3076 - accuracy: 0.5289 - val_loss: 1.5841 - val_accuracy: 0.4538\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3000 - accuracy: 0.5343 - val_loss: 1.5542 - val_accuracy: 0.4584\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.3015 - accuracy: 0.5310 - val_loss: 1.5297 - val_accuracy: 0.4640\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.2806 - accuracy: 0.5414 - val_loss: 1.5242 - val_accuracy: 0.4650\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.2690 - accuracy: 0.5478 - val_loss: 1.5443 - val_accuracy: 0.4622\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.2699 - accuracy: 0.5405 - val_loss: 1.5474 - val_accuracy: 0.4564\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.2610 - accuracy: 0.5486 - val_loss: 1.5719 - val_accuracy: 0.4554\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2486 - accuracy: 0.5539 - val_loss: 1.5376 - val_accuracy: 0.4638\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2427 - accuracy: 0.5545 - val_loss: 1.5504 - val_accuracy: 0.4708\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2448 - accuracy: 0.5553 - val_loss: 1.5664 - val_accuracy: 0.4544\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2310 - accuracy: 0.5621 - val_loss: 1.5678 - val_accuracy: 0.4542\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2238 - accuracy: 0.5625 - val_loss: 1.5561 - val_accuracy: 0.4666\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.2023 - accuracy: 0.5722 - val_loss: 1.5740 - val_accuracy: 0.4544\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.2105 - accuracy: 0.5669 - val_loss: 1.5715 - val_accuracy: 0.4582\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.2025 - accuracy: 0.5694 - val_loss: 1.5571 - val_accuracy: 0.4650\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.2011 - accuracy: 0.5697 - val_loss: 1.5565 - val_accuracy: 0.4636\n",
      "Epoch 50/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.1913 - accuracy: 0.5746 - val_loss: 1.5882 - val_accuracy: 0.4640\n",
      "Epoch 51/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1876 - accuracy: 0.5738 - val_loss: 1.5844 - val_accuracy: 0.4632\n",
      "Epoch 52/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1808 - accuracy: 0.5778 - val_loss: 1.5771 - val_accuracy: 0.4628\n",
      "Epoch 53/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1691 - accuracy: 0.5826 - val_loss: 1.5507 - val_accuracy: 0.4664\n",
      "Epoch 54/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1590 - accuracy: 0.5844 - val_loss: 1.5548 - val_accuracy: 0.4672\n",
      "Epoch 55/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1601 - accuracy: 0.5889 - val_loss: 1.5583 - val_accuracy: 0.4776\n",
      "Epoch 56/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1392 - accuracy: 0.5944 - val_loss: 1.5792 - val_accuracy: 0.4702\n",
      "Epoch 57/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.1376 - accuracy: 0.5899 - val_loss: 1.5978 - val_accuracy: 0.4676\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14dbf3eb0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "capable-second",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 1s 3ms/step - loss: 1.5242 - accuracy: 0.4650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.524153470993042, 0.4650000035762787]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = keras.models.load_model(\"my_cifar10_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-holmes",
   "metadata": {},
   "source": [
    "##### c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "accepted-physics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 45s 22ms/step - loss: 1.9729 - accuracy: 0.2988 - val_loss: 1.6748 - val_accuracy: 0.3976\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 27s 20ms/step - loss: 1.6785 - accuracy: 0.4023 - val_loss: 1.5861 - val_accuracy: 0.4264\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.6066 - accuracy: 0.4304 - val_loss: 1.5326 - val_accuracy: 0.4436\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.5483 - accuracy: 0.4497 - val_loss: 1.5017 - val_accuracy: 0.4646\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.5017 - accuracy: 0.4651 - val_loss: 1.4163 - val_accuracy: 0.4982\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.4593 - accuracy: 0.4836 - val_loss: 1.4197 - val_accuracy: 0.4910\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.4293 - accuracy: 0.4969 - val_loss: 1.4305 - val_accuracy: 0.4918\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.4055 - accuracy: 0.5034 - val_loss: 1.3619 - val_accuracy: 0.5166\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.3763 - accuracy: 0.5160 - val_loss: 1.3703 - val_accuracy: 0.5102\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 1.3527 - accuracy: 0.5178 - val_loss: 1.3495 - val_accuracy: 0.5222\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 1.3233 - accuracy: 0.5325 - val_loss: 1.3548 - val_accuracy: 0.5252\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.3039 - accuracy: 0.5404 - val_loss: 1.3778 - val_accuracy: 0.5074\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.2880 - accuracy: 0.5451 - val_loss: 1.3486 - val_accuracy: 0.5182\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.2631 - accuracy: 0.5529 - val_loss: 1.3485 - val_accuracy: 0.5302\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 25s 17ms/step - loss: 1.2577 - accuracy: 0.5565 - val_loss: 1.3480 - val_accuracy: 0.5256\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.2388 - accuracy: 0.5593 - val_loss: 1.3316 - val_accuracy: 0.5316\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 29s 20ms/step - loss: 1.2225 - accuracy: 0.5676 - val_loss: 1.3295 - val_accuracy: 0.5400\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.2056 - accuracy: 0.5723 - val_loss: 1.3454 - val_accuracy: 0.5260\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 35s 25ms/step - loss: 1.1970 - accuracy: 0.5786 - val_loss: 1.3488 - val_accuracy: 0.5210\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 38s 27ms/step - loss: 1.1858 - accuracy: 0.5832 - val_loss: 1.3409 - val_accuracy: 0.5354\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1665 - accuracy: 0.5881 - val_loss: 1.3751 - val_accuracy: 0.5206\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.1531 - accuracy: 0.5920 - val_loss: 1.3436 - val_accuracy: 0.5296\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1414 - accuracy: 0.6002 - val_loss: 1.3278 - val_accuracy: 0.5336\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1214 - accuracy: 0.6061 - val_loss: 1.3346 - val_accuracy: 0.5378\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.1179 - accuracy: 0.6090 - val_loss: 1.3381 - val_accuracy: 0.5338\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.1017 - accuracy: 0.6081 - val_loss: 1.3662 - val_accuracy: 0.5248\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 33s 24ms/step - loss: 1.0906 - accuracy: 0.6156 - val_loss: 1.3318 - val_accuracy: 0.5410\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0795 - accuracy: 0.6180 - val_loss: 1.3638 - val_accuracy: 0.5284\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0779 - accuracy: 0.6169 - val_loss: 1.3269 - val_accuracy: 0.5434\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.0672 - accuracy: 0.6237 - val_loss: 1.3657 - val_accuracy: 0.5390\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 30s 22ms/step - loss: 1.0559 - accuracy: 0.6266 - val_loss: 1.3365 - val_accuracy: 0.5464\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0327 - accuracy: 0.6370 - val_loss: 1.3594 - val_accuracy: 0.5432\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0313 - accuracy: 0.6342 - val_loss: 1.3677 - val_accuracy: 0.5440\n",
      "Epoch 34/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 1.0163 - accuracy: 0.6426 - val_loss: 1.3642 - val_accuracy: 0.5364\n",
      "Epoch 35/100\n",
      "1407/1407 [==============================] - 32s 23ms/step - loss: 1.0165 - accuracy: 0.6410 - val_loss: 1.3689 - val_accuracy: 0.5332\n",
      "Epoch 36/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 1.0059 - accuracy: 0.6447 - val_loss: 1.3482 - val_accuracy: 0.5402\n",
      "Epoch 37/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9860 - accuracy: 0.6553 - val_loss: 1.3658 - val_accuracy: 0.5360\n",
      "Epoch 38/100\n",
      "1407/1407 [==============================] - 30s 21ms/step - loss: 0.9816 - accuracy: 0.6552 - val_loss: 1.3738 - val_accuracy: 0.5444\n",
      "Epoch 39/100\n",
      "1407/1407 [==============================] - 27s 19ms/step - loss: 0.9870 - accuracy: 0.6503 - val_loss: 1.3835 - val_accuracy: 0.5410\n",
      "Epoch 40/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9673 - accuracy: 0.6590 - val_loss: 1.4121 - val_accuracy: 0.5262\n",
      "Epoch 41/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9549 - accuracy: 0.6638 - val_loss: 1.3650 - val_accuracy: 0.5484\n",
      "Epoch 42/100\n",
      "1407/1407 [==============================] - 31s 22ms/step - loss: 0.9525 - accuracy: 0.6667 - val_loss: 1.3827 - val_accuracy: 0.5380\n",
      "Epoch 43/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9512 - accuracy: 0.6619 - val_loss: 1.3864 - val_accuracy: 0.5430\n",
      "Epoch 44/100\n",
      "1407/1407 [==============================] - 32s 22ms/step - loss: 0.9338 - accuracy: 0.6736 - val_loss: 1.3796 - val_accuracy: 0.5364\n",
      "Epoch 45/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9208 - accuracy: 0.6757 - val_loss: 1.4263 - val_accuracy: 0.5390\n",
      "Epoch 46/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9065 - accuracy: 0.6806 - val_loss: 1.3964 - val_accuracy: 0.5442\n",
      "Epoch 47/100\n",
      "1407/1407 [==============================] - 29s 21ms/step - loss: 0.9042 - accuracy: 0.6835 - val_loss: 1.3936 - val_accuracy: 0.5434\n",
      "Epoch 48/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.9100 - accuracy: 0.6796 - val_loss: 1.3977 - val_accuracy: 0.5350\n",
      "Epoch 49/100\n",
      "1407/1407 [==============================] - 28s 20ms/step - loss: 0.8953 - accuracy: 0.6831 - val_loss: 1.4135 - val_accuracy: 0.5364\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.3269 - accuracy: 0.5434\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.3268986940383911, 0.54339998960495]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer=\"he_normal\"))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation(\"elu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_bn_model.h5\", save_best_only=True)\n",
    "run_index = 2 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_bn_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "model.fit(X_train, y_train, epochs=100,\n",
    "          validation_data=(X_valid, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_bn_model.h5\")\n",
    "model.evaluate(X_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "average-marks",
   "metadata": {},
   "source": [
    "##### d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "warming-adobe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 31s 16ms/step - loss: 2.0806 - accuracy: 0.2568 - val_loss: 1.8298 - val_accuracy: 0.3382\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 1.7541 - accuracy: 0.3756 - val_loss: 1.8512 - val_accuracy: 0.3396\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6417 - accuracy: 0.4211 - val_loss: 1.6536 - val_accuracy: 0.3922\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.5431 - accuracy: 0.4563 - val_loss: 1.6655 - val_accuracy: 0.4174\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4984 - accuracy: 0.4737 - val_loss: 1.5808 - val_accuracy: 0.4504\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.4417 - accuracy: 0.4917 - val_loss: 1.5568 - val_accuracy: 0.4516\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.4010 - accuracy: 0.5108 - val_loss: 1.5379 - val_accuracy: 0.4616\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.3455 - accuracy: 0.5310 - val_loss: 1.4958 - val_accuracy: 0.4828\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.3276 - accuracy: 0.5398 - val_loss: 1.5028 - val_accuracy: 0.4656\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.2877 - accuracy: 0.5493 - val_loss: 1.5193 - val_accuracy: 0.4872\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2603 - accuracy: 0.5695 - val_loss: 1.5345 - val_accuracy: 0.4920\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.2308 - accuracy: 0.5748 - val_loss: 1.4957 - val_accuracy: 0.4894\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.2074 - accuracy: 0.5816 - val_loss: 1.4892 - val_accuracy: 0.4938\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 1.1722 - accuracy: 0.5967 - val_loss: 1.4984 - val_accuracy: 0.4968\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.2819 - accuracy: 0.5588 - val_loss: 1.5474 - val_accuracy: 0.4796\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2696 - accuracy: 0.5573 - val_loss: 1.5517 - val_accuracy: 0.4836\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.1943 - accuracy: 0.5792 - val_loss: 1.5004 - val_accuracy: 0.4990\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 1.1565 - accuracy: 0.6023 - val_loss: 1.5049 - val_accuracy: 0.4952\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 19s 14ms/step - loss: 1.1225 - accuracy: 0.6148 - val_loss: 1.5153 - val_accuracy: 0.5088\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.0901 - accuracy: 0.6285 - val_loss: 1.4973 - val_accuracy: 0.4980\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.0719 - accuracy: 0.6297 - val_loss: 1.4993 - val_accuracy: 0.4994\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.3222 - accuracy: 0.6237 - val_loss: 1.6101 - val_accuracy: 0.4524\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 22s 16ms/step - loss: 1.2713 - accuracy: 0.5497 - val_loss: 1.5240 - val_accuracy: 0.4796\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1419 - accuracy: 0.6015 - val_loss: 1.5101 - val_accuracy: 0.5002\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0945 - accuracy: 0.6177 - val_loss: 1.5281 - val_accuracy: 0.5020\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 23s 16ms/step - loss: 1.0519 - accuracy: 0.6386 - val_loss: 1.5434 - val_accuracy: 0.4944\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 24s 17ms/step - loss: 1.0256 - accuracy: 0.6496 - val_loss: 1.5265 - val_accuracy: 0.5088\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9931 - accuracy: 0.6603 - val_loss: 1.5325 - val_accuracy: 0.4958\n",
      "Epoch 29/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9887 - accuracy: 0.6629 - val_loss: 1.6042 - val_accuracy: 0.5008\n",
      "Epoch 30/100\n",
      "1407/1407 [==============================] - 25s 18ms/step - loss: 0.9773 - accuracy: 0.6644 - val_loss: 1.5830 - val_accuracy: 0.4980\n",
      "Epoch 31/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9700 - accuracy: 0.6647 - val_loss: 1.5634 - val_accuracy: 0.5010\n",
      "Epoch 32/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9524 - accuracy: 0.6725 - val_loss: 1.5744 - val_accuracy: 0.5076\n",
      "Epoch 33/100\n",
      "1407/1407 [==============================] - 19s 13ms/step - loss: 0.9581 - accuracy: 0.6727 - val_loss: 1.6001 - val_accuracy: 0.5096\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4892 - accuracy: 0.4938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4892140626907349, 0.49380001425743103]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=7e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_selu_model.h5\", save_best_only=True)\n",
    "run_index = 3 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_selu_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_selu_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-wheel",
   "metadata": {},
   "source": [
    "##### e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "stuck-exception",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1407/1407 [==============================] - 27s 16ms/step - loss: 2.0527 - accuracy: 0.2834 - val_loss: 1.7384 - val_accuracy: 0.3922\n",
      "Epoch 2/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.6687 - accuracy: 0.4070 - val_loss: 1.7031 - val_accuracy: 0.4020\n",
      "Epoch 3/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.5785 - accuracy: 0.4453 - val_loss: 1.6444 - val_accuracy: 0.4192\n",
      "Epoch 4/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.5039 - accuracy: 0.4691 - val_loss: 1.6284 - val_accuracy: 0.4458\n",
      "Epoch 5/100\n",
      "1407/1407 [==============================] - 18s 13ms/step - loss: 1.4480 - accuracy: 0.4945 - val_loss: 1.6178 - val_accuracy: 0.4656\n",
      "Epoch 6/100\n",
      "1407/1407 [==============================] - 16s 12ms/step - loss: 1.4004 - accuracy: 0.5092 - val_loss: 1.5052 - val_accuracy: 0.4686\n",
      "Epoch 7/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.3518 - accuracy: 0.5305 - val_loss: 1.5366 - val_accuracy: 0.4812\n",
      "Epoch 8/100\n",
      "1407/1407 [==============================] - 17s 12ms/step - loss: 1.3095 - accuracy: 0.5401 - val_loss: 1.4890 - val_accuracy: 0.4938\n",
      "Epoch 9/100\n",
      "1407/1407 [==============================] - 16s 11ms/step - loss: 1.2715 - accuracy: 0.5573 - val_loss: 1.5252 - val_accuracy: 0.4812\n",
      "Epoch 10/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2341 - accuracy: 0.5691 - val_loss: 1.4951 - val_accuracy: 0.4814\n",
      "Epoch 11/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.2002 - accuracy: 0.5862 - val_loss: 1.5366 - val_accuracy: 0.4998\n",
      "Epoch 12/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1784 - accuracy: 0.5979 - val_loss: 1.5606 - val_accuracy: 0.4938\n",
      "Epoch 13/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.1436 - accuracy: 0.6042 - val_loss: 1.5718 - val_accuracy: 0.5038\n",
      "Epoch 14/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1178 - accuracy: 0.6137 - val_loss: 1.5166 - val_accuracy: 0.5098\n",
      "Epoch 15/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0911 - accuracy: 0.6216 - val_loss: 1.5993 - val_accuracy: 0.5136\n",
      "Epoch 16/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 1.0708 - accuracy: 0.6314 - val_loss: 1.6099 - val_accuracy: 0.5104\n",
      "Epoch 17/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0471 - accuracy: 0.6379 - val_loss: 1.6289 - val_accuracy: 0.5082\n",
      "Epoch 18/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0251 - accuracy: 0.6482 - val_loss: 1.6785 - val_accuracy: 0.5054\n",
      "Epoch 19/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 1.0031 - accuracy: 0.6567 - val_loss: 1.6534 - val_accuracy: 0.5078\n",
      "Epoch 20/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9747 - accuracy: 0.6672 - val_loss: 1.6756 - val_accuracy: 0.5104\n",
      "Epoch 21/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 0.9652 - accuracy: 0.6699 - val_loss: 1.6365 - val_accuracy: 0.5120\n",
      "Epoch 22/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 0.9483 - accuracy: 0.6785 - val_loss: 1.6527 - val_accuracy: 0.5032\n",
      "Epoch 23/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 0.9995 - accuracy: 0.6697 - val_loss: 1.6501 - val_accuracy: 0.4506\n",
      "Epoch 24/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.2862 - accuracy: 0.5516 - val_loss: 1.6066 - val_accuracy: 0.4898\n",
      "Epoch 25/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.1347 - accuracy: 0.6069 - val_loss: 1.6366 - val_accuracy: 0.4962\n",
      "Epoch 26/100\n",
      "1407/1407 [==============================] - 20s 14ms/step - loss: 1.0283 - accuracy: 0.6426 - val_loss: 1.6939 - val_accuracy: 0.4888\n",
      "Epoch 27/100\n",
      "1407/1407 [==============================] - 21s 15ms/step - loss: 0.9739 - accuracy: 0.6647 - val_loss: 1.6772 - val_accuracy: 0.5122\n",
      "Epoch 28/100\n",
      "1407/1407 [==============================] - 20s 15ms/step - loss: 0.9273 - accuracy: 0.6825 - val_loss: 1.7276 - val_accuracy: 0.5094\n",
      "157/157 [==============================] - 1s 3ms/step - loss: 1.4890 - accuracy: 0.4938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4890233278274536, 0.49380001425743103]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.Nadam(lr=5e-4)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_cifar10_alpha_dropout_model.h5\", save_best_only=True)\n",
    "run_index = 1 # increment every time you train the model\n",
    "run_logdir = os.path.join(os.curdir, \"my_cifar10_logs\", \"run_alpha_dropout_{:03d}\".format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    "\n",
    "X_means = X_train.mean(axis=0)\n",
    "X_stds = X_train.std(axis=0)\n",
    "X_train_scaled = (X_train - X_means) / X_stds\n",
    "X_valid_scaled = (X_valid - X_means) / X_stds\n",
    "X_test_scaled = (X_test - X_means) / X_stds\n",
    "\n",
    "model.fit(X_train_scaled, y_train, epochs=100,\n",
    "          validation_data=(X_valid_scaled, y_valid),\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"my_cifar10_alpha_dropout_model.h5\")\n",
    "model.evaluate(X_valid_scaled, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dominant-vessel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "moved-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aware-anaheim",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probas(mc_model, X, n_samples=10):\n",
    "    Y_probas = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probas, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probas = mc_dropout_predict_probas(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probas, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "proper-consumer",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4946"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid_scaled)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decreased-scottish",
   "metadata": {},
   "source": [
    "##### f."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "domestic-encyclopedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100,\n",
    "                                 kernel_initializer=\"lecun_normal\",\n",
    "                                 activation=\"selu\"))\n",
    "\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation=\"softmax\"))\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=1e-3)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "sunrise-compact",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "352/352 [==============================] - 4s 10ms/step - loss: nan - accuracy: 0.1364\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.615227699279785,\n",
       " 2.6200389862060547,\n",
       " 3.9805657523018976)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(keras.backend.get_value(self.model.optimizer.lr))\n",
    "        self.losses.append(logs[\"loss\"])\n",
    "        keras.backend.set_value(self.model.optimizer.lr, self.model.optimizer.lr * self.factor)\n",
    "\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = keras.backend.get_value(model.optimizer.lr)\n",
    "    keras.backend.set_value(model.optimizer.lr, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size,\n",
    "                        callbacks=[exp_lr])\n",
    "    keras.backend.set_value(model.optimizer.lr, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "batch_size = 128\n",
    "rates, losses = find_learning_rate(model, X_train_scaled, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "trying-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        keras.backend.set_value(self.model.optimizer.lr, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "natural-inspiration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "352/352 [==============================] - 5s 15ms/step - loss: 2.0559 - accuracy: 0.2811 - val_loss: 1.8059 - val_accuracy: 0.3804\n",
      "Epoch 2/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.7649 - accuracy: 0.3738 - val_loss: 1.6721 - val_accuracy: 0.4166\n",
      "Epoch 3/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.6200 - accuracy: 0.4254 - val_loss: 1.6323 - val_accuracy: 0.4256\n",
      "Epoch 4/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.5395 - accuracy: 0.4551 - val_loss: 1.5964 - val_accuracy: 0.4394\n",
      "Epoch 5/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.4951 - accuracy: 0.4724 - val_loss: 1.5912 - val_accuracy: 0.4564\n",
      "Epoch 6/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.4524 - accuracy: 0.4847 - val_loss: 1.5454 - val_accuracy: 0.4604\n",
      "Epoch 7/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.4115 - accuracy: 0.4980 - val_loss: 1.6541 - val_accuracy: 0.4404\n",
      "Epoch 8/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.3498 - accuracy: 0.5195 - val_loss: 1.4743 - val_accuracy: 0.4912\n",
      "Epoch 9/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 1.2739 - accuracy: 0.5460 - val_loss: 1.5355 - val_accuracy: 0.4788\n",
      "Epoch 10/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.2060 - accuracy: 0.5703 - val_loss: 1.5121 - val_accuracy: 0.5094\n",
      "Epoch 11/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 1.1323 - accuracy: 0.5952 - val_loss: 1.5457 - val_accuracy: 0.4992\n",
      "Epoch 12/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 1.0629 - accuracy: 0.6181 - val_loss: 1.5095 - val_accuracy: 0.5110\n",
      "Epoch 13/15\n",
      "352/352 [==============================] - 4s 12ms/step - loss: 0.9916 - accuracy: 0.6435 - val_loss: 1.5130 - val_accuracy: 0.5320\n",
      "Epoch 14/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 0.9260 - accuracy: 0.6684 - val_loss: 1.5229 - val_accuracy: 0.5348\n",
      "Epoch 15/15\n",
      "352/352 [==============================] - 4s 11ms/step - loss: 0.8869 - accuracy: 0.6829 - val_loss: 1.5484 - val_accuracy: 0.5380\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train_scaled) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(X_train_scaled, y_train, epochs=n_epochs, batch_size=batch_size,\n",
    "                    validation_data=(X_valid_scaled, y_valid),\n",
    "                    callbacks=[onecycle])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
